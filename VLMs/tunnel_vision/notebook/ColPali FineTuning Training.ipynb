{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User is already logged in.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import io\n",
    "import torch\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from typing import cast\n",
    "from pathlib import Path\n",
    "from peft import LoraConfig\n",
    "from huggingface_hub import notebook_login\n",
    "from colpali_engine.loss import ColbertPairwiseCELoss\n",
    "from colpali_engine.models import ColPali, ColPaliProcessor\n",
    "from datasets import load_from_disk, Dataset, Features, Image, Value\n",
    "from colpali_engine.trainer.contrastive_trainer import ContrastiveTrainer\n",
    "from colpali_engine.utils.torch_utils import get_torch_device, tear_down_torch\n",
    "from colpali_engine.collators.visual_retriever_collator import VisualRetrieverCollator\n",
    "from transformers import BitsAndBytesConfig, TrainerCallback, TrainingArguments, EarlyStoppingCallback, logging\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "warnings.simplefilter('ignore')\n",
    "notebook_login(new_session=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path: /home/dgarieck23/VLMs/tunnel_vision\n"
     ]
    }
   ],
   "source": [
    "wd = os.path.dirname(os.getcwd())\n",
    "os.chdir(wd)\n",
    "print(f'path: {wd}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model: nn.Module) -> None:\n",
    "    '''\n",
    "    Print the number of trainable parameters in the model.\n",
    "    '''\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f'trainable params: {trainable_params:,} || all params: {all_param:,} || trainable%: {100 * trainable_params / all_param}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leveraging GPU for Perfomance\n",
    "To optimize performance, we'll use GPU accelaration if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n",
      "GPU name: NVIDIA GeForce RTX 4090 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Seed\n",
    "To enhance reproductibility, and comparatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "random.seed(seed)              # Python's built-in random module\n",
    "np.random.seed(seed)           # NumPy\n",
    "torch.manual_seed(seed)        # PyTorch\n",
    "torch.cuda.manual_seed(seed)   # For GPU computations in PyTorch\n",
    "torch.cuda.manual_seed_all(seed)  # If you're using multiple GPUs\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose a quantization strategy\n",
    "\n",
    "Quantization not only reduces the size of model weights but also reduces the memory consumption during inference. Below is an example of how memory usage changes with different quantization strategies:\n",
    "\n",
    "- **fp16 (16-bit floating point)**: Each parameter requires 16 bits (2 bytes).\n",
    "- **8-bit Quantization**: Each parameter requires 8 bits (1 byte).\n",
    "- **4-bit Quantization**: Each parameter requires 4 bits (0.5 bytes).\n",
    "\n",
    "#### Example:\n",
    "\n",
    "Assume a model has **100 million parameters**.\n",
    "\n",
    "| Precision         | Bits per Parameter | Total Memory Usage (MB) |\n",
    "|-------------------|--------------------|-------------------------|\n",
    "| **fp16 (16-bit)** | 16 bits (2 bytes)  | 100M * 2 bytes = 200 MB |\n",
    "| **8-bit**         | 8 bits (1 byte)    | 100M * 1 byte = 100 MB  |\n",
    "| **4-bit**         | 4 bits (0.5 bytes) | 100M * 0.5 bytes = 50 MB|\n",
    "\n",
    "As seen from the table, moving from fp16 to 8-bit cuts the memory usage in half, and further reducing to 4-bit halves the memory usage again. Quantization thus plays a significant role in reducing the memory footprint of models, which is especially useful for deploying large models on devices with limited memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_strat = '4bit'\n",
    "\n",
    "if quant_strat and str(device) != 'cuda:0':\n",
    "    raise ValueError('This notebook requires a CUDA GPU to use quantization.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare quantization config\n",
    "if quant_strat is None:\n",
    "    bnb_config = None\n",
    "elif quant_strat == '8bit':\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,\n",
    "    )\n",
    "elif quant_strat == '4bit':\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(f'Invalid quantization strategy: {quant_strat}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the pre-trained model\n",
    "\n",
    "ColPali is an advanced retrieval model that combines two powerful components:\n",
    "- **Col**: Derived from ColBERT (Contextualized Late Interaction over BERT), a retrieval method designed for efficient and accurate document search by embedding queries and document passages into high-dimensional spaces and applying a late interaction mechanism for matching.\n",
    "- **Pali**: Refers to a Vision Language Model (VLM) named PaliGemma, which is used to process visual content such as images or screenshots from documents.\n",
    "\n",
    "##### How ColPali Works\n",
    "\n",
    "Instead of relying on complex text-based PDF parsing, ColPali simplifies the indexing process by using **screenshots** of PDF pages. These visual representations are embedded using the Vision Language Model (PaliGemma). When a query is provided at inference time, ColPali embeds the query and matches it to the most similar document pages using the **contextualized late interaction mechanism** introduced by ColBERT. This enables ColPali to efficiently retrieve the most relevant document pages based on the visual and contextual similarity of the content.\n",
    "\n",
    "\n",
    "<div>\n",
    "<p style=\"text-align: left;\">Architecture of ColPali illustrated</p>\n",
    "<img src=\"/home/dgarieck23/VLMs/tunnel_vision/misc/colpali_arch.png\" width=\"400\" height=\"300\"/>\n",
    "<p style=\"text-align: left;\"><em>Source: https://x.com/helloiamleonie</em></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-trained model name (with LoRA adapter)\n",
    "model_name = 'vidore/colpali-v1.2'\n",
    "\n",
    "# get the LoRA config from the pretrained model\n",
    "lora_config = LoraConfig.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecbf9ed1a87f4e1cad59e8c61eb4ec01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the model with the loaded pre-trained adapter\n",
    "model = cast(\n",
    "    ColPali,\n",
    "    ColPali.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=device,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model.active_adapters():\n",
    "    raise ValueError('No adapter found in the model.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deciding the Fine-tuning strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When adapting a pre-trained model to a new task, we need to decide on the fine-tuning strategy. Two common approaches are fine-tuning the bias terms or using LoRA (Low-Rank Adaptation), both of which allow us to modify the model without adjusting all of its parameters.\n",
    "\n",
    "- **Fine-tuning the bias** involves updating only the bias terms of the model, which are small, additional parameters added to each neuron. This method is lightweight and quick, as only a small portion of the model’s parameters are changed. It’s useful for slight adaptations to the model when working with a related task or dataset. However, this strategy has limited flexibility since only biases are adjusted, which may not be sufficient for more complex tasks.\n",
    "\n",
    "- **LoRA (Low-Rank Adaptation)** is a more flexible fine-tuning approach that introduces small, trainable matrices (adapters) into specific layers of the model. These matrices enable significant adjustments without modifying the core weights of the pre-trained model. LoRA provides a balance between preserving the model’s original knowledge and allowing it to learn new tasks, while still being memory-efficient compared to full fine-tuning.\n",
    "\n",
    "The choice of strategy depends on the specific task and resources. If minimal adjustment is required, bias fine-tuning is a quick and effective option. For tasks needing more model flexibility while maintaining the original pre-trained knowledge, LoRA provides a more powerful fine-tuning solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 39,292,928 || all params: 1,766,287,216 || trainable%: 2.224605808390791\n"
     ]
    }
   ],
   "source": [
    "# we need to unfreeze tha parameters\n",
    "params_name = 'lora' # either bias or lora\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if params_name in name:\n",
    "        param.requires_grad = True\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the processor and the collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lora_config.base_model_name_or_path is None:\n",
    "    raise ValueError('Base model name or path is required in the LoRA config.')\n",
    "\n",
    "processor = cast(\n",
    "    ColPaliProcessor,\n",
    "    ColPaliProcessor.from_pretrained(model_name),\n",
    ")\n",
    "collator = VisualRetrieverCollator(processor=processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk('data/processed/annual reports')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'query', 'answer'],\n",
       "        num_rows: 3428\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'query', 'answer'],\n",
       "        num_rows: 1470\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_dir = Path('models/checkpoints')\n",
    "checkpoints_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=str(checkpoints_dir),\n",
    "    hub_model_id=None,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1.5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=False,\n",
    "    eval_strategy='steps',\n",
    "    save_steps=200,\n",
    "    logging_steps=20,\n",
    "    eval_steps=100,\n",
    "    warmup_steps=100,\n",
    "    learning_rate=5e-5,\n",
    "    save_total_limit=1,\n",
    "    report_to=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluateFirstStepCallback(TrainerCallback):\n",
    "    '''\n",
    "    Run eval after the first training step.\n",
    "    Used to have a more precise evaluation learning curve.\n",
    "    '''\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step == 1:\n",
    "            control.should_evaluate = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = ContrastiveTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    args=training_args,\n",
    "    data_collator=collator,\n",
    "    loss_func=ColbertPairwiseCELoss(),\n",
    "    is_vision_model=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.args.remove_unused_columns = False\n",
    "trainer.add_callback(EvaluateFirstStepCallback())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune the model\n",
    "\n",
    "Unfortunately, I ran out of memory while trying to fine-tune the model locally. To work around this limitation, I attempted fine-tuning only the bias parameters, but this did not yield significant results for my task. However, the good news is that you can still use this notebook to train your own model by following the steps outlined here. If you have access to more powerful hardware, you could explore more comprehensive fine-tuning strategies, such as using LoRA or even full fine-tuning, depending on your needs and resources.\n",
    "\n",
    "\n",
    "<div>\n",
    "<p style=\"text-align: left;\">Large Language Models vs GPUs</p>\n",
    "<img src=\"/home/dgarieck23/VLMs/tunnel_vision/misc/llm_vs_gpu.png\" width=\"400\" height=\"300\"/>\n",
    "<p style=\"text-align: left;\"><em>Source: Tom & Jerry</em></p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "train_results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the model adapter \n",
    "trainer.save_model('models/colpali_ar_finetuned_4bit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load your fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_name = 'models/colpali_ar_finetuned_4bit'\n",
    "\n",
    "model = cast(\n",
    "    ColPali,\n",
    "    ColPali.from_pretrained(\n",
    "        adapter_name,\n",
    "        quantization_config=bnb_config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=device,\n",
    "    ),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tunnel-vision-jesxmyu7-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
