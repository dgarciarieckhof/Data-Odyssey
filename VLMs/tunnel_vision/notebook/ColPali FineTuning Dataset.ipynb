{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User is already logged in.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import pprint\n",
    "import random\n",
    "import warnings\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from PIL import Image\n",
    "from uuid import uuid4\n",
    "from tqdm import tqdm, trange\n",
    "from pydantic import BaseModel\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from pdf2image import convert_from_path\n",
    "from dataclasses import asdict, dataclass\n",
    "from datasets import Dataset, load_dataset\n",
    "from huggingface_hub import notebook_login\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "notebook_login(new_session=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path: /home/dgarieck23/VLMs/tunnel_vision\n"
     ]
    }
   ],
   "source": [
    "wd = os.path.dirname(os.getcwd())\n",
    "os.chdir(wd)\n",
    "print(f'path: {wd}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leveraging GPU for Perfomance\n",
    "To optimize performance, we'll use GPU accelaration if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n",
      "GPU name: NVIDIA GeForce RTX 4090 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Seed\n",
    "To enhance reproductibility, and comparatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "random.seed(seed)              # python's built-in random module\n",
    "np.random.seed(seed)           # numPy\n",
    "torch.manual_seed(seed)        # pyTorch\n",
    "torch.cuda.manual_seed(seed)   # for GPU computations in PyTorch\n",
    "torch.cuda.manual_seed_all(seed)  # if you're using multiple GPUs\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pdf to Image\n",
    "\n",
    "Performs a batch conversion of PDF files into images. It reads the files from a specified directory, converts each page into an image, and saves the resulting images in a temporary folder. Saving the images into a temporary folder will help us to avoid weird [bugs](https://github.com/huggingface/datasets/issues/4796) loading the dataset for fine-tuning using the library datasets from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:30<00:00, 10.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images saved in temporary folder: /tmp/tmpepb88zcr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "temp_dir = tempfile.mkdtemp()\n",
    "inputs = 'data/raw/annual reports/'\n",
    "pdf_files = os.listdir('data/raw/annual reports/')\n",
    "\n",
    "\n",
    "for file in tqdm(pdf_files):\n",
    "    # convert the PDF pages to images\n",
    "    images = convert_from_path(f'{inputs}{file}', dpi=100, thread_count=6)\n",
    "    \n",
    "    # save each image with a unique name in the temporary directory\n",
    "    for idx, img in enumerate(images):\n",
    "        img_filename = f\"{os.path.splitext(file)[0]}_page_{idx + 1}.png\"\n",
    "        img.save(os.path.join(temp_dir, img_filename))\n",
    "\n",
    "print(f'Images saved in temporary folder: {temp_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the images saved in the temporary directory as a dataset, using the load_dataset function from the Hugging Face datasets library. The dataset is structured in an image folder format, where each image file is treated as an individual data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d4e08bb7e5944298b818a79f3d2760b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/817 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18be25677be343ea90a47c8c9477c7a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/817 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "248ed998902f45d8b35604563252aa1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('imagefolder', data_dir=temp_dir, split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Qwen2-VL to generate queries\n",
    "\n",
    "[Qwen-VL](https://qwen2vl.com/), developed by Alibaba Cloud, is a visual multimodal model from the Qwen series designed to handle inputs like images, text, and bounding boxes, producing text and bounding box outputs.\n",
    "\n",
    "Key Features:\n",
    "- **Superior Performance**: Outperforms other similar models on benchmarks like Zero-shot Captioning, VQA, DocVQA, and Grounding.\n",
    "- **Multilingual Text Recognition**: Especially strong in recognizing bilingual text (Chinese and English) in images.\n",
    "- **Multi-Image Conversations**: Enables comparison and storytelling across multiple images.\n",
    "- **High-Resolution Understanding**: Operates at a higher resolution (448 vs. 224), enhancing tasks like fine-grained recognition and document QA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vl_model = 'Qwen/Qwen2-VL-2B-Instruct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\n",
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2af3c45d8cf048e98d97c86f6887d1dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    vl_model,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation='flash_attention_2',\n",
    "    device_map=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default processer\n",
    "processor = AutoProcessor.from_pretrained(vl_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building ColPali Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pydantic Models\n",
    "The use of Pydantic ensures that the data in the queries are consistently structured and validated, improving reliability when the system processes complex and varied inputs. Each model corresponds to a distinct query type and includes fields for both the query and its explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneralRetrievalQuery(BaseModel):\n",
    "    broad_topical_query: str\n",
    "    broad_topical_explanation: str\n",
    "    specific_detail_query: str\n",
    "    specific_detail_explanation: str\n",
    "    visual_element_query: str\n",
    "    visual_element_explanation: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiDocumentComparisonQuery(BaseModel):\n",
    "    comparison_query: str\n",
    "    comparison_explanation: str\n",
    "    corroboration_contradiction_query: str\n",
    "    corroboration_contradiction_explanation: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DomainSpecificQuery(BaseModel):\n",
    "    identified_domain: str\n",
    "    domain_specific_query: str\n",
    "    domain_specific_explanation: str\n",
    "    data_findings_query: str\n",
    "    data_findings_explanation: str\n",
    "    applications_implications_query: str\n",
    "    applications_implications_explanation: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualElementFocusQuery(BaseModel):\n",
    "    similar_visual_element_query: str\n",
    "    similar_visual_element_explanation: str\n",
    "    text_visual_combination_query: str\n",
    "    text_visual_combination_explanation: str\n",
    "    visual_content_understanding_query: str\n",
    "    visual_content_understanding_explanation: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalMetadataQuery(BaseModel):\n",
    "    temporal_query: str\n",
    "    temporal_explanation: str\n",
    "    topic_metadata_combination_query: str\n",
    "    topic_metadata_combination_explanation: str\n",
    "    update_related_document_query: str\n",
    "    update_related_document_explanation: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DifficultyAmbiguityQuery(BaseModel):\n",
    "    simple_query: str\n",
    "    simple_explanation: str\n",
    "    complex_query: str\n",
    "    complex_explanation: str\n",
    "    ambiguous_query: str\n",
    "    ambiguous_explanation: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilingualMultimodalQuery(BaseModel):\n",
    "    multilingual_query: str\n",
    "    multilingual_explanation: str\n",
    "    multimodal_combination_query: str\n",
    "    multimodal_combination_explanation: str\n",
    "    text_visual_understanding_query: str\n",
    "    text_visual_understanding_explanation: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prompting\n",
    "\n",
    "Different prompts are created based on the Pydantic models to generate multiple query sets, each based on distinct ideas or aspects. The dataset will be built around seven different templates, each focusing on a different dimension or type of query. These templates allow the system to cover a broad range of scenarios and use cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gral_template = '''\n",
    "\n",
    "You are an AI assistant specialized in document retrieval tasks within the financial domain, specifically for annual reports. Given an image of a document page, your task is to generate retrieval queries that someone might use to find this financial document in a large corpus of reports.\n",
    "\n",
    "Please generate 3 different types of retrieval queries:\n",
    "\n",
    "1. A broad topical query: This should cover the main subject of the document, such as financial performance, key company metrics, or strategic initiatives.\n",
    "2. A specific detail query: This should focus on a particular fact, financial figure (e.g., revenue, net profit), or specific point made in the document.\n",
    "3. A visual element query: This should reference a chart, financial graph, or other visual components such as balance sheets or income statements, if present.\n",
    "\n",
    "Important guidelines:\n",
    "- Ensure the queries are relevant for retrieval tasks, particularly focusing on financial data, and not just describing the page content.\n",
    "- Frame the queries as if someone is searching for this financial document in a corpus of reports, not asking questions about its content.\n",
    "- Make the queries diverse and representative of different search strategies, including financial terms and specific company performance indicators.\n",
    "\n",
    "For each query, also provide a brief explanation of why this query would be effective in retrieving this financial document.\n",
    "\n",
    "Format your response as a JSON object with the following structure:\n",
    "\n",
    "{\n",
    "  \"broad_topical_query\": \"Your query here\",\n",
    "  \"broad_topical_explanation\": \"Brief explanation\",\n",
    "  \"specific_detail_query\": \"Your query here\",\n",
    "  \"specific_detail_explanation\": \"Brief explanation\",\n",
    "  \"visual_element_query\": \"Your query here\",\n",
    "  \"visual_element_explanation\": \"Brief explanation\"\n",
    "}\n",
    "\n",
    "If there are no relevant visual elements, replace the third query with another specific detail query that references financial data.\n",
    "\n",
    "Here is the document image to analyze:\n",
    "<image>\n",
    "\n",
    "Generate the queries based on this image and provide the response in the specified JSON format.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_template = '''\n",
    "\n",
    "Imagine this financial document page is part of a larger corpus of annual reports. Your task is to generate retrieval queries that would require comparing this document with others in the corpus, particularly focusing on financial data, company performance, and market trends.\n",
    "\n",
    "Please generate 2 retrieval queries:\n",
    "\n",
    "1. A query comparing this document’s financial performance, trends, or metrics with a related subject, such as performance from a different year, a competitor's report, or industry benchmarks.\n",
    "2. A query seeking documents that either contradict or support the financial figures, strategies, or statements made in this document (e.g., conflicting market trends, opposing financial analyses, or differing growth projections).\n",
    "\n",
    "For each query, provide a brief explanation of how it encourages document comparison and why it would be effective for retrieval within a financial corpus.\n",
    "\n",
    "Format your response as a JSON object with the following structure:\n",
    "\n",
    "{\n",
    "  \"comparison_query\": \"Your query here\",\n",
    "  \"comparison_explanation\": \"Brief explanation\",\n",
    "  \"corroboration_contradiction_query\": \"Your query here\",\n",
    "  \"corroboration_contradiction_explanation\": \"Brief explanation\"\n",
    "}\n",
    "\n",
    "Here is the document image to analyze:\n",
    "<image>\n",
    "\n",
    "Generate the queries based on this image and provide the response in the specified JSON format.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dom_template = '''\n",
    "Your task is to create retrieval queries that a financial professional or analyst might use to find this document in a large corpus of financial documents, specifically annual reports.\n",
    "\n",
    "First, identify the domain of the document as \"financial.\"\n",
    "\n",
    "Then, generate 3 retrieval queries:\n",
    "\n",
    "1. A query using domain-specific terminology, such as key financial metrics, accounting terms, or industry-specific jargon.\n",
    "2. A query seeking specific financial data or findings presented in the document, such as revenue, net income, cash flow, or key performance indicators (KPIs).\n",
    "3. A query related to the document’s potential applications or implications, such as its relevance to investment decisions, market positioning, or future growth strategies.\n",
    "\n",
    "For each query, provide a brief explanation of its relevance to the financial domain and why it would be effective for retrieval in a corpus of annual reports.\n",
    "\n",
    "Format your response as a JSON object with the following structure:\n",
    "\n",
    "{\n",
    "  \"identified_domain\": \"financial\",\n",
    "  \"domain_specific_query\": \"Your query here\",\n",
    "  \"domain_specific_explanation\": \"Brief explanation\",\n",
    "  \"data_findings_query\": \"Your query here\",\n",
    "  \"data_findings_explanation\": \"Brief explanation\",\n",
    "  \"applications_implications_query\": \"Your query here\",\n",
    "  \"applications_implications_explanation\": \"Brief explanation\"\n",
    "}\n",
    "\n",
    "Here is the document image to analyze:\n",
    "<image>\n",
    "\n",
    "Generate the queries based on this image and provide the response in the specified JSON format.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_template = '''\n",
    "Your task is to generate retrieval queries focusing on the visual elements present in this financial document page (such as financial charts, tables, graphs, or diagrams).\n",
    "\n",
    "Please generate 3 retrieval queries:\n",
    "\n",
    "1. A query specifically asking for documents with similar financial visual elements, such as bar charts of revenue trends, pie charts of market share, or financial tables (e.g., balance sheets, income statements).\n",
    "2. A query combining textual and visual financial information, such as connecting financial figures in the text (e.g., revenue, net income) with their representation in graphs or tables.\n",
    "3. A query that would require understanding the content of the financial visual element, such as interpreting the performance trend in a line chart or analyzing the relationship between metrics in a financial table, to retrieve this document.\n",
    "\n",
    "For each query, provide a brief explanation of how it incorporates financial visual elements and why it would be effective for retrieval in a financial corpus.\n",
    "\n",
    "Format your response as a JSON object with the following structure:\n",
    "\n",
    "{\n",
    "  \"similar_visual_element_query\": \"Your query here\",\n",
    "  \"similar_visual_element_explanation\": \"Brief explanation\",\n",
    "  \"text_visual_combination_query\": \"Your query here\",\n",
    "  \"text_visual_combination_explanation\": \"Brief explanation\",\n",
    "  \"visual_content_understanding_query\": \"Your query here\",\n",
    "  \"visual_content_understanding_explanation\": \"Brief explanation\"\n",
    "}\n",
    "\n",
    "If the document lacks significant visual elements, explain this and generate alternative queries focusing on the financial document's structure or layout (e.g., section headings, data tables).\n",
    "\n",
    "Here is the document image to analyze:\n",
    "<image>\n",
    "\n",
    "Generate the queries based on this image and provide the response in the specified JSON format.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_template = '''\n",
    "Assuming this financial document is part of a large, diverse corpus of annual reports, your task is to generate retrieval queries that incorporate metadata or temporal aspects relevant to financial reporting.\n",
    "\n",
    "Please generate 3 retrieval queries:\n",
    "\n",
    "1. A query specifying a likely time frame for this document, such as the fiscal year or publication date (e.g., \"2023 annual report\" or \"Q4 financial report\").\n",
    "2. A query combining financial topical information (e.g., revenue, net income) with a metadata element, such as the company name, report type (e.g., balance sheet, income statement), or auditor name.\n",
    "3. A query seeking updated or related financial reports on the same topic, such as subsequent reports from the same company or financial updates for the same fiscal year.\n",
    "\n",
    "For each query, provide a brief explanation of how it uses temporal or metadata information and why it would be effective for retrieving financial documents.\n",
    "\n",
    "Format your response as a JSON object with the following structure:\n",
    "\n",
    "{\n",
    "  \"temporal_query\": \"Your query here\",\n",
    "  \"temporal_explanation\": \"Brief explanation\",\n",
    "  \"topic_metadata_combination_query\": \"Your query here\",\n",
    "  \"topic_metadata_combination_explanation\": \"Brief explanation\",\n",
    "  \"update_related_document_query\": \"Your query here\",\n",
    "  \"update_related_document_explanation\": \"Brief explanation\"\n",
    "}\n",
    "\n",
    "Here is the document image to analyze:\n",
    "<image>\n",
    "\n",
    "Generate the queries based on this image and provide the response in the specified JSON format.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_template = '''\n",
    "Your task is to create retrieval queries for this financial document, considering different levels of complexity and ambiguity, which reflect common information retrieval tasks in a corpus of financial reports.\n",
    "\n",
    "Please generate 3 retrieval queries:\n",
    "\n",
    "1. A simple, straightforward query focused on a single aspect of the document, such as a key financial figure (e.g., revenue, net income).\n",
    "2. A complex query that requires understanding multiple financial metrics, trends, or sections of the document (e.g., linking financial performance with strategic initiatives or multiple sections like balance sheets and income statements).\n",
    "3. An ambiguous query that could retrieve this document among others, possibly due to a more general term (e.g., \"annual financial performance\" or \"company revenue trends\"), which could apply to many documents in the corpus.\n",
    "\n",
    "For each query, provide a brief explanation of its complexity level or ambiguity and why it would be effective or challenging for retrieval in the context of financial documents.\n",
    "\n",
    "Format your response as a JSON object with the following structure:\n",
    "\n",
    "{\n",
    "  \"simple_query\": \"Your query here\",\n",
    "  \"simple_explanation\": \"Brief explanation\",\n",
    "  \"complex_query\": \"Your query here\",\n",
    "  \"complex_explanation\": \"Brief explanation\",\n",
    "  \"ambiguous_query\": \"Your query here\",\n",
    "  \"ambiguous_explanation\": \"Brief explanation\"\n",
    "}\n",
    "\n",
    "Here is the document image to analyze:\n",
    "<image>\n",
    "\n",
    "Generate the queries based on this image and provide the response in the specified JSON format.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mll_template = '''\n",
    "Your task is to generate retrieval queries considering potential multilingual and multi-modal aspects of this financial document.\n",
    "\n",
    "Please generate 3 retrieval queries:\n",
    "\n",
    "1. A query in a different language (if applicable) that would retrieve this financial document (e.g., a query in Spanish, French, or another relevant language).\n",
    "2. A query combining textual financial data (e.g., revenue, net income) with non-textual elements like charts, graphs, or tables representing this data visually.\n",
    "3. A query that requires understanding both the financial text and visual elements (e.g., interpreting financial performance from text descriptions and visualizing trends in a graph or table) to retrieve this document accurately.\n",
    "\n",
    "For each query, provide a brief explanation of its multilingual or multi-modal nature and why it would be effective for retrieving financial documents.\n",
    "\n",
    "Format your response as a JSON object with the following structure:\n",
    "\n",
    "{\n",
    "  \"multilingual_query\": \"Your query here\",\n",
    "  \"multilingual_explanation\": \"Brief explanation\",\n",
    "  \"multimodal_combination_query\": \"Your query here\",\n",
    "  \"multimodal_combination_explanation\": \"Brief explanation\",\n",
    "  \"text_visual_understanding_query\": \"Your query here\",\n",
    "  \"text_visual_understanding_explanation\": \"Brief explanation\"\n",
    "}\n",
    "\n",
    "If the document is not suitable for multilingual queries, explain why and provide an alternative query that focuses on the financial structure or visual layout.\n",
    "\n",
    "Here is the document image to analyze:\n",
    "<image>\n",
    "\n",
    "Generate the queries based on this image and provide the response in the specified JSON format.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retrieval_prompt(prompt_name: str,) -> Tuple[str, Union[\n",
    "        GeneralRetrievalQuery,\n",
    "        MultiDocumentComparisonQuery,\n",
    "        DomainSpecificQuery,\n",
    "        VisualElementFocusQuery,\n",
    "        TemporalMetadataQuery,\n",
    "        DifficultyAmbiguityQuery,\n",
    "        MultilingualMultimodalQuery,\n",
    "    ],\n",
    "]:\n",
    "    prompts = {\n",
    "        \"general\": (gral_template,GeneralRetrievalQuery),\n",
    "        \"comparison\": (comp_template,MultiDocumentComparisonQuery),\n",
    "        \"domain\": (dom_template,DomainSpecificQuery),\n",
    "        \"visual\": (vis_template,VisualElementFocusQuery),\n",
    "        \"temporal\": (temp_template,TemporalMetadataQuery),\n",
    "        \"difficulty\": (diff_template,DifficultyAmbiguityQuery),\n",
    "        \"multilingual\": (mll_template, MultilingualMultimodalQuery),\n",
    "    }\n",
    "\n",
    "    if prompt_name not in prompts:\n",
    "        raise ValueError(\n",
    "            f\"Invalid prompt name. Choose from: {', '.join(prompts.keys())}\"\n",
    "        )\n",
    "\n",
    "    return prompts[prompt_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating ColPali Queries\n",
    "\n",
    "The following function generates a multimodal response by combining text (prompt) and image inputs. It uses a pretrained visual-language model (Qwen) and processor to interpret the input and generate a response, which will be useful for our visual question answering task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt, image):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"image\": image,\n",
    "                },\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=400)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :]\n",
    "        for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False,\n",
    "    )\n",
    "\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating Queries\n",
    "\n",
    "Queries are generated for a set of images by using predefined prompt templates and corresponding Pydantic models. Each query is tailored to a specific query type, and the system generates responses based on both the image and the prompt.\n",
    "\n",
    "For the purposes of this notebook, only **general queries** will be generated and run through inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating queries with general template: 100%|██████████| 817/817 [1:03:45<00:00,  4.68s/it]\n"
     ]
    }
   ],
   "source": [
    "prompts = {} \n",
    "images = dataset['image']\n",
    "\n",
    "for prompt_name in ['general']:\n",
    "    prompt, pydantic_model = get_retrieval_prompt(prompt_name)\n",
    "    responses = []\n",
    "    for image in tqdm(images, desc=f'Generating queries with {prompt_name} template: '):\n",
    "        try:\n",
    "            resp = generate_response(prompt, image)\n",
    "            responses.append(resp)\n",
    "        except Exception as e:\n",
    "            responses.append(None)\n",
    "    prompts[prompt_name] = responses    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing Queries Into a Dataset\n",
    "\n",
    "A custom function is used to parse the responses (which are in a JSON-like format) into a Python dictionary. The function ensures that all expected keys from the JSON-like string are included in the final output. If a key contains an invalid or incomplete value, the function assigns None to that key, ensuring consistent structure and completeness in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_string_to_dict(s):\n",
    "    \"\"\"\n",
    "    Parses a JSON-like string into a dictionary, ensuring all keys are included.\n",
    "    Assigns None to keys with invalid or incomplete values.\n",
    "\n",
    "    Args:\n",
    "        s (str): The input string containing key-value pairs.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with all keys from the input string. Valid values are assigned,\n",
    "              and None is assigned to keys with invalid or incomplete values.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # regular expression patterns\n",
    "        key_pattern = r'\"([^\"\\\\]*(?:\\\\.[^\"\\\\]*)*)\"\\s*:'  # matches keys\n",
    "        kv_pattern = r'\"([^\"\\\\]*(?:\\\\.[^\"\\\\]*)*)\"\\s*:\\s*\"([^\"\\\\]*(?:\\\\.[^\"\\\\]*)*)\"'  # matches valid \"key\": \"value\" pairs\n",
    "\n",
    "        # extract all keys\n",
    "        keys = re.findall(key_pattern, s)\n",
    "        # Unescape any escaped characters in keys\n",
    "        keys = [bytes(key, \"utf-8\").decode(\"unicode_escape\") for key in keys]\n",
    "\n",
    "        # extract valid key-value pairs\n",
    "        valid_kv_matches = re.findall(kv_pattern, s)\n",
    "        valid_kv = {}\n",
    "        for key, value in valid_kv_matches:\n",
    "            try:\n",
    "                # unescape any escaped characters\n",
    "                unescaped_key = bytes(key, \"utf-8\").decode(\"unicode_escape\")\n",
    "                unescaped_value = bytes(value, \"utf-8\").decode(\"unicode_escape\")\n",
    "                valid_kv[unescaped_key] = unescaped_value\n",
    "            except UnicodeDecodeError:\n",
    "                # if decoding fails, skip this key-value pair\n",
    "                continue\n",
    "\n",
    "        # construct the final dictionary\n",
    "        final_dict = {}\n",
    "        for key in keys:\n",
    "            if key in valid_kv:\n",
    "                final_dict[key] = valid_kv[key]\n",
    "            else:\n",
    "                final_dict[key] = None\n",
    "\n",
    "        return final_dict\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of None responses in general: 0\n"
     ]
    }
   ],
   "source": [
    "for prompt_name in ['general']:\n",
    "    print(f'number of None responses in {prompt_name}: {len([r for r in prompts[prompt_name] if r is None])}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_qa = prompts['general']\n",
    "general_qa = [qa[0].replace('```','').replace('json','').replace('\\n','') for qa in general_qa]\n",
    "general_qa = [parse_string_to_dict(qa) for qa in general_qa]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.add_column(name='queries',column=general_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode_queries(batch):\n",
    "    images = []\n",
    "    queries = []\n",
    "    answers = []\n",
    "\n",
    "    # loop over each row in the batch\n",
    "    for i in range(len(batch['queries'])):\n",
    "        # extract the current dictionary of queries for each row\n",
    "        for query, answer in batch['queries'][i].items():\n",
    "            if answer is not None:  # only add entries where answer is not None\n",
    "                images.append(batch['image'][i])  # append the corresponding image\n",
    "                queries.append(query)\n",
    "                answers.append(answer)\n",
    "\n",
    "    return {'image': images, 'query': queries, 'answer': answers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75323c998b60435385038dcfb59a93b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/817 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "exploded_dataset = dataset.map(explode_queries, batched=True, remove_columns=['queries'])\n",
    "exploded_dataset = exploded_dataset.train_test_split(test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'query', 'answer'],\n",
       "        num_rows: 3428\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'query', 'answer'],\n",
       "        num_rows: 1470\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect output\n",
    "exploded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "610eb36d926d4be6bc14a366d81869ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/3428 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1a124ac95134ef1b0cd3824c43eb24d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1470 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# store output\n",
    "file_name = 'data/processed/annual reports'\n",
    "exploded_dataset.save_to_disk(file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vrag-YONw0x_s-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
